# DQN para Navegaci√≥n en Rob√≥tica M√≥vil (ROS 2)

Este repositorio contiene la implementaci√≥n de un sistema de navegaci√≥n aut√≥noma para un robot m√≥vil utilizando Deep Q-Networks (DQN) en un entorno de simulaci√≥n basado en ROS 2 y Stage.
El agente es entrenado para desplazarse hacia un objetivo evitando colisiones con obst√°culos, aprendiendo una pol√≠tica de control a partir de datos sensoriales (LiDAR y odometr√≠a).

## Descripci√≥n del Proyecto

El proyecto aborda el problema de navegaci√≥n aut√≥noma mediante aprendizaje por refuerzo, donde el robot:

Percibe el entorno mediante un sensor LiDAR.

Recibe informaci√≥n relativa al objetivo (distancia y √°ngulo).

Ejecuta acciones discretas de movimiento (avance y giros).

Aprende a maximizar una funci√≥n de recompensa que incentiva el progreso hacia el objetivo y penaliza colisiones.

Durante el entrenamiento, los objetivos se generan de forma aleatoria en zonas libres de obst√°culos, utilizando el mapa del entorno para evitar posiciones inv√°lidas.
Posteriormente, el sistema es evaluado en una fase de prueba independiente para analizar su desempe√±o y tasa de √©xito.

## Enfoque Metodol√≥gico

Algoritmo: Deep Q-Network (DQN)

Red neuronal: MLP (MLPRegressor ‚Äì scikit-learn)

Entradas del estado:

LiDAR discretizado en bins

Distancia al objetivo

√Ångulo relativo al objetivo

Espacio de acciones: 5 acciones discretas (avance, giro y combinaciones)

Entorno: Simulaci√≥n en Stage con ROS 2

Entrenamiento: Exploraci√≥n Œµ-greedy y replay buffer

## Requisitos

Ubuntu 20.04 / 22.04

ROS 2 (Humble / Jazzy)

Python 3.8+

Dependencias Python:

pip install numpy scikit-learn

## Uso del Proyecto
1Ô∏è‚É£ Entrenamiento del agente
ros2 run dqn_project train_node


El agente se entrena en el entorno de simulaci√≥n generando objetivos aleatorios en zonas v√°lidas del mapa.

2Ô∏è‚É£ Pruebas del modelo entrenado
ros2 run dqn_final test_node /home/grace/Documents/Diplomado_Robotica/16_01_ws/assets/model_final.pkl


Se eval√∫a el desempe√±o del agente entrenado midiendo √©xito y colisiones.

## Resultados

En las pruebas finales realizadas, el sistema alcanz√≥ un success rate del 60%, con una tasa de colisi√≥n del 40% en 10 ejecuciones independientes.
Estos resultados indican que el agente aprendi√≥ una pol√≠tica de navegaci√≥n funcional, aunque a√∫n existen oportunidades de mejora para reducir colisiones en entornos m√°s complejos.

## Anexos

üîó Repositorio GitHub:
https://github.com/GraceLunaVerdueta/dqn_robotica_movil

## Video de demostraci√≥n:
(https://drive.google.com/drive/folders/174T2z81lO862zKzThYr7oSV7aSs2ivac?usp=sharing)

## Autores

Grace Luna Verdueta
Daniel Alcazar Salas
Santiago Aguilera Salinas
Sandro Murillo Quispe

## Licencia

Este proyecto se desarrolla con fines acad√©micos.
El uso y modificaci√≥n del c√≥digo es libre para investigaci√≥n y aprendizaje, citando la fuente correspondiente.
